{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "ffD6f3g8dqe9",
        "outputId": "e9a81be0-c7f8-4752-d58f-d8a321b33192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 22.7 MB 1.4 MB/s \n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ae45c44-3f40-454f-98a7-7626228651f5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8ae45c44-3f40-454f-98a7-7626228651f5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving gandata.csv to gandata.csv\n",
            "(191392,)\n",
            "(63798,)\n",
            "{'=', 'N', 'B', '3', '5', '4', '7', 's', '2', '(', 'c', '8', 'r', '#', '[', 'F', 'E', '+', ']', '1', 'o', '9', '6', '-', 'l', 'H', '!', 'C', 'n', 'O', 'S', ')', 'I'}\n",
            "CC1Cc2c(Br)cccc2N1C(=O)Cc1nc(N2CCOCC2)cc(=O)n1C\n",
            "(191392, 130, 33)\n",
            "!CC1Cc2c(Br)cccc2N1C(=O)Cc1nc(N2CCOCC2)cc(=O)n1CEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 130, 33)]    0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        296960      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 512)          0           ['lstm[0][1]',                   \n",
            "                                                                  'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          65664       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 130, 33)]    0           []                               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          33024       ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          33024       ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 130, 256)     296960      ['input_2[0][0]',                \n",
            "                                                                  'dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 130, 33)      8481        ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 734,113\n",
            "Trainable params: 734,113\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "187/187 [==============================] - 51s 233ms/step - loss: 0.8695 - accuracy: 0.7450 - val_loss: 0.5704 - val_accuracy: 0.8140 - lr: 0.0050\n",
            "Epoch 2/200\n",
            "187/187 [==============================] - 44s 235ms/step - loss: 0.5109 - accuracy: 0.8304 - val_loss: 0.4588 - val_accuracy: 0.8478 - lr: 0.0050\n",
            "Epoch 3/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.4350 - accuracy: 0.8542 - val_loss: 0.4097 - val_accuracy: 0.8610 - lr: 0.0050\n",
            "Epoch 4/200\n",
            "187/187 [==============================] - 45s 243ms/step - loss: 0.3985 - accuracy: 0.8643 - val_loss: 0.3809 - val_accuracy: 0.8695 - lr: 0.0050\n",
            "Epoch 5/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.3647 - accuracy: 0.8747 - val_loss: 0.3481 - val_accuracy: 0.8799 - lr: 0.0050\n",
            "Epoch 6/200\n",
            "187/187 [==============================] - 46s 244ms/step - loss: 0.3387 - accuracy: 0.8824 - val_loss: 0.3266 - val_accuracy: 0.8860 - lr: 0.0050\n",
            "Epoch 7/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.3221 - accuracy: 0.8872 - val_loss: 0.3126 - val_accuracy: 0.8903 - lr: 0.0050\n",
            "Epoch 8/200\n",
            "187/187 [==============================] - 46s 244ms/step - loss: 0.3103 - accuracy: 0.8909 - val_loss: 0.3026 - val_accuracy: 0.8934 - lr: 0.0050\n",
            "Epoch 9/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.2984 - accuracy: 0.8948 - val_loss: 0.2899 - val_accuracy: 0.8978 - lr: 0.0050\n",
            "Epoch 10/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.2877 - accuracy: 0.8984 - val_loss: 0.2854 - val_accuracy: 0.8995 - lr: 0.0050\n",
            "Epoch 11/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.2757 - accuracy: 0.9028 - val_loss: 0.2646 - val_accuracy: 0.9073 - lr: 0.0050\n",
            "Epoch 12/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.2633 - accuracy: 0.9075 - val_loss: 0.2521 - val_accuracy: 0.9117 - lr: 0.0050\n",
            "Epoch 13/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.2526 - accuracy: 0.9114 - val_loss: 0.2518 - val_accuracy: 0.9121 - lr: 0.0050\n",
            "Epoch 14/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.2406 - accuracy: 0.9158 - val_loss: 0.2395 - val_accuracy: 0.9166 - lr: 0.0050\n",
            "Epoch 15/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.2331 - accuracy: 0.9187 - val_loss: 0.2276 - val_accuracy: 0.9208 - lr: 0.0050\n",
            "Epoch 16/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.2241 - accuracy: 0.9220 - val_loss: 0.2195 - val_accuracy: 0.9238 - lr: 0.0050\n",
            "Epoch 17/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.2172 - accuracy: 0.9245 - val_loss: 0.2183 - val_accuracy: 0.9243 - lr: 0.0050\n",
            "Epoch 18/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.2082 - accuracy: 0.9277 - val_loss: 0.2060 - val_accuracy: 0.9290 - lr: 0.0050\n",
            "Epoch 19/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.2015 - accuracy: 0.9303 - val_loss: 0.1953 - val_accuracy: 0.9328 - lr: 0.0050\n",
            "Epoch 20/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1962 - accuracy: 0.9323 - val_loss: 0.1934 - val_accuracy: 0.9335 - lr: 0.0050\n",
            "Epoch 21/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1903 - accuracy: 0.9344 - val_loss: 0.1991 - val_accuracy: 0.9311 - lr: 0.0050\n",
            "Epoch 22/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1861 - accuracy: 0.9359 - val_loss: 0.1855 - val_accuracy: 0.9363 - lr: 0.0050\n",
            "Epoch 23/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1792 - accuracy: 0.9383 - val_loss: 0.1808 - val_accuracy: 0.9378 - lr: 0.0050\n",
            "Epoch 24/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1757 - accuracy: 0.9397 - val_loss: 0.1730 - val_accuracy: 0.9412 - lr: 0.0050\n",
            "Epoch 25/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1722 - accuracy: 0.9409 - val_loss: 0.1749 - val_accuracy: 0.9403 - lr: 0.0050\n",
            "Epoch 26/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1697 - accuracy: 0.9418 - val_loss: 0.1706 - val_accuracy: 0.9418 - lr: 0.0050\n",
            "Epoch 27/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1670 - accuracy: 0.9429 - val_loss: 0.1770 - val_accuracy: 0.9391 - lr: 0.0050\n",
            "Epoch 28/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1639 - accuracy: 0.9440 - val_loss: 0.1632 - val_accuracy: 0.9448 - lr: 0.0050\n",
            "Epoch 29/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1606 - accuracy: 0.9453 - val_loss: 0.1757 - val_accuracy: 0.9400 - lr: 0.0050\n",
            "Epoch 30/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1555 - accuracy: 0.9471 - val_loss: 0.1560 - val_accuracy: 0.9475 - lr: 0.0050\n",
            "Epoch 31/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1532 - accuracy: 0.9479 - val_loss: 0.1555 - val_accuracy: 0.9475 - lr: 0.0050\n",
            "Epoch 32/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1488 - accuracy: 0.9495 - val_loss: 0.1510 - val_accuracy: 0.9492 - lr: 0.0050\n",
            "Epoch 33/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1500 - accuracy: 0.9493 - val_loss: 0.1481 - val_accuracy: 0.9505 - lr: 0.0050\n",
            "Epoch 34/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1425 - accuracy: 0.9517 - val_loss: 0.1522 - val_accuracy: 0.9487 - lr: 0.0050\n",
            "Epoch 35/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1425 - accuracy: 0.9518 - val_loss: 0.1489 - val_accuracy: 0.9498 - lr: 0.0050\n",
            "Epoch 36/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1402 - accuracy: 0.9526 - val_loss: 0.1433 - val_accuracy: 0.9520 - lr: 0.0050\n",
            "Epoch 37/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1360 - accuracy: 0.9540 - val_loss: 0.1405 - val_accuracy: 0.9531 - lr: 0.0050\n",
            "Epoch 38/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1353 - accuracy: 0.9544 - val_loss: 0.1403 - val_accuracy: 0.9530 - lr: 0.0050\n",
            "Epoch 39/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1337 - accuracy: 0.9549 - val_loss: 0.1400 - val_accuracy: 0.9531 - lr: 0.0050\n",
            "Epoch 40/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1316 - accuracy: 0.9557 - val_loss: 0.1360 - val_accuracy: 0.9546 - lr: 0.0050\n",
            "Epoch 41/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1279 - accuracy: 0.9568 - val_loss: 0.1338 - val_accuracy: 0.9553 - lr: 0.0050\n",
            "Epoch 42/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1278 - accuracy: 0.9570 - val_loss: 0.1313 - val_accuracy: 0.9562 - lr: 0.0050\n",
            "Epoch 43/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1233 - accuracy: 0.9584 - val_loss: 0.1309 - val_accuracy: 0.9564 - lr: 0.0050\n",
            "Epoch 44/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1254 - accuracy: 0.9578 - val_loss: 0.1298 - val_accuracy: 0.9567 - lr: 0.0050\n",
            "Epoch 45/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1246 - accuracy: 0.9581 - val_loss: 0.1262 - val_accuracy: 0.9582 - lr: 0.0050\n",
            "Epoch 46/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1199 - accuracy: 0.9596 - val_loss: 0.1262 - val_accuracy: 0.9581 - lr: 0.0050\n",
            "Epoch 47/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1191 - accuracy: 0.9600 - val_loss: 0.1272 - val_accuracy: 0.9576 - lr: 0.0050\n",
            "Epoch 48/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1153 - accuracy: 0.9613 - val_loss: 0.1264 - val_accuracy: 0.9579 - lr: 0.0050\n",
            "Epoch 49/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1184 - accuracy: 0.9602 - val_loss: 0.1226 - val_accuracy: 0.9594 - lr: 0.0050\n",
            "Epoch 50/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1135 - accuracy: 0.9619 - val_loss: 0.1519 - val_accuracy: 0.9501 - lr: 0.0050\n",
            "Epoch 51/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1119 - accuracy: 0.9623 - val_loss: 0.1203 - val_accuracy: 0.9602 - lr: 0.0050\n",
            "Epoch 52/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1144 - accuracy: 0.9617 - val_loss: 0.1363 - val_accuracy: 0.9541 - lr: 0.0050\n",
            "Epoch 53/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1094 - accuracy: 0.9633 - val_loss: 0.1310 - val_accuracy: 0.9561 - lr: 0.0050\n",
            "Epoch 54/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1082 - accuracy: 0.9637 - val_loss: 0.1282 - val_accuracy: 0.9573 - lr: 0.0050\n",
            "Epoch 55/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1088 - accuracy: 0.9634 - val_loss: 0.1144 - val_accuracy: 0.9623 - lr: 0.0050\n",
            "Epoch 56/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1182 - accuracy: 0.9602 - val_loss: 0.1250 - val_accuracy: 0.9580 - lr: 0.0050\n",
            "Epoch 57/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1105 - accuracy: 0.9629 - val_loss: 0.1140 - val_accuracy: 0.9624 - lr: 0.0050\n",
            "Epoch 58/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1046 - accuracy: 0.9649 - val_loss: 0.1140 - val_accuracy: 0.9624 - lr: 0.0050\n",
            "Epoch 59/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1056 - accuracy: 0.9646 - val_loss: 0.1255 - val_accuracy: 0.9577 - lr: 0.0050\n",
            "Epoch 60/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1042 - accuracy: 0.9651 - val_loss: 0.1352 - val_accuracy: 0.9546 - lr: 0.0050\n",
            "Epoch 61/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1040 - accuracy: 0.9651 - val_loss: 0.1175 - val_accuracy: 0.9607 - lr: 0.0050\n",
            "Epoch 62/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0983 - accuracy: 0.9670 - val_loss: 0.1168 - val_accuracy: 0.9613 - lr: 0.0050\n",
            "Epoch 63/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.1020 - accuracy: 0.9658 - val_loss: 0.1068 - val_accuracy: 0.9648 - lr: 0.0050\n",
            "Epoch 64/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.0997 - accuracy: 0.9665 - val_loss: 0.1058 - val_accuracy: 0.9652 - lr: 0.0050\n",
            "Epoch 65/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0955 - accuracy: 0.9679 - val_loss: 0.1079 - val_accuracy: 0.9643 - lr: 0.0050\n",
            "Epoch 66/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.1033 - accuracy: 0.9655 - val_loss: 0.1270 - val_accuracy: 0.9571 - lr: 0.0050\n",
            "Epoch 67/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0969 - accuracy: 0.9674 - val_loss: 0.1110 - val_accuracy: 0.9631 - lr: 0.0050\n",
            "Epoch 68/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0965 - accuracy: 0.9678 - val_loss: 0.1065 - val_accuracy: 0.9647 - lr: 0.0050\n",
            "Epoch 69/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0961 - accuracy: 0.9678 - val_loss: 0.1019 - val_accuracy: 0.9665 - lr: 0.0050\n",
            "Epoch 70/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0921 - accuracy: 0.9691 - val_loss: 0.1031 - val_accuracy: 0.9661 - lr: 0.0050\n",
            "Epoch 71/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0949 - accuracy: 0.9681 - val_loss: 0.1008 - val_accuracy: 0.9669 - lr: 0.0050\n",
            "Epoch 72/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.0912 - accuracy: 0.9694 - val_loss: 0.1064 - val_accuracy: 0.9648 - lr: 0.0050\n",
            "Epoch 73/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.0894 - accuracy: 0.9699 - val_loss: 0.0999 - val_accuracy: 0.9672 - lr: 0.0050\n",
            "Epoch 74/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0910 - accuracy: 0.9694 - val_loss: 0.1025 - val_accuracy: 0.9662 - lr: 0.0050\n",
            "Epoch 75/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0959 - accuracy: 0.9680 - val_loss: 0.0997 - val_accuracy: 0.9671 - lr: 0.0050\n",
            "Epoch 76/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0869 - accuracy: 0.9709 - val_loss: 0.0982 - val_accuracy: 0.9678 - lr: 0.0050\n",
            "Epoch 77/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0948 - accuracy: 0.9680 - val_loss: 0.0968 - val_accuracy: 0.9682 - lr: 0.0050\n",
            "Epoch 78/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0887 - accuracy: 0.9703 - val_loss: 0.1189 - val_accuracy: 0.9610 - lr: 0.0050\n",
            "Epoch 79/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0842 - accuracy: 0.9718 - val_loss: 0.0951 - val_accuracy: 0.9688 - lr: 0.0050\n",
            "Epoch 80/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0870 - accuracy: 0.9708 - val_loss: 0.0955 - val_accuracy: 0.9687 - lr: 0.0050\n",
            "Epoch 81/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0865 - accuracy: 0.9710 - val_loss: 0.1046 - val_accuracy: 0.9656 - lr: 0.0050\n",
            "Epoch 82/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.0819 - accuracy: 0.9725 - val_loss: 0.0937 - val_accuracy: 0.9693 - lr: 0.0050\n",
            "Epoch 83/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0887 - accuracy: 0.9704 - val_loss: 0.0918 - val_accuracy: 0.9701 - lr: 0.0050\n",
            "Epoch 84/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0852 - accuracy: 0.9714 - val_loss: 0.0923 - val_accuracy: 0.9698 - lr: 0.0050\n",
            "Epoch 85/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0795 - accuracy: 0.9734 - val_loss: 0.0932 - val_accuracy: 0.9694 - lr: 0.0050\n",
            "Epoch 86/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0889 - accuracy: 0.9703 - val_loss: 0.0905 - val_accuracy: 0.9705 - lr: 0.0050\n",
            "Epoch 87/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0783 - accuracy: 0.9738 - val_loss: 0.0955 - val_accuracy: 0.9687 - lr: 0.0050\n",
            "Epoch 88/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0794 - accuracy: 0.9733 - val_loss: 0.0914 - val_accuracy: 0.9700 - lr: 0.0050\n",
            "Epoch 89/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0852 - accuracy: 0.9716 - val_loss: 0.0938 - val_accuracy: 0.9692 - lr: 0.0050\n",
            "Epoch 90/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.0793 - accuracy: 0.9734 - val_loss: 0.0885 - val_accuracy: 0.9712 - lr: 0.0050\n",
            "Epoch 91/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0800 - accuracy: 0.9733 - val_loss: 0.0894 - val_accuracy: 0.9708 - lr: 0.0050\n",
            "Epoch 92/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0758 - accuracy: 0.9746 - val_loss: 0.0892 - val_accuracy: 0.9709 - lr: 0.0050\n",
            "Epoch 93/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0802 - accuracy: 0.9731 - val_loss: 0.0906 - val_accuracy: 0.9703 - lr: 0.0050\n",
            "Epoch 94/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0828 - accuracy: 0.9724 - val_loss: 0.0942 - val_accuracy: 0.9690 - lr: 0.0050\n",
            "Epoch 95/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0846 - accuracy: 0.9720 - val_loss: 0.1024 - val_accuracy: 0.9657 - lr: 0.0050\n",
            "Epoch 96/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0744 - accuracy: 0.9751 - val_loss: 0.0890 - val_accuracy: 0.9709 - lr: 0.0050\n",
            "Epoch 97/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0738 - accuracy: 0.9753 - val_loss: 0.0857 - val_accuracy: 0.9721 - lr: 0.0050\n",
            "Epoch 98/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0798 - accuracy: 0.9734 - val_loss: 0.0869 - val_accuracy: 0.9717 - lr: 0.0050\n",
            "Epoch 99/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0740 - accuracy: 0.9752 - val_loss: 0.0994 - val_accuracy: 0.9670 - lr: 0.0050\n",
            "Epoch 100/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0742 - accuracy: 0.9751 - val_loss: 0.0859 - val_accuracy: 0.9720 - lr: 0.0050\n",
            "Epoch 101/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0769 - accuracy: 0.9743 - val_loss: 0.0850 - val_accuracy: 0.9724 - lr: 0.0050\n",
            "Epoch 102/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0702 - accuracy: 0.9765 - val_loss: 0.0976 - val_accuracy: 0.9679 - lr: 0.0050\n",
            "Epoch 103/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0847 - accuracy: 0.9719 - val_loss: 0.0906 - val_accuracy: 0.9701 - lr: 0.0050\n",
            "Epoch 104/200\n",
            "187/187 [==============================] - 46s 245ms/step - loss: 0.0693 - accuracy: 0.9769 - val_loss: 0.0834 - val_accuracy: 0.9729 - lr: 0.0050\n",
            "Epoch 105/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0682 - accuracy: 0.9772 - val_loss: 0.0828 - val_accuracy: 0.9731 - lr: 0.0050\n",
            "Epoch 106/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0705 - accuracy: 0.9763 - val_loss: 0.0819 - val_accuracy: 0.9734 - lr: 0.0050\n",
            "Epoch 107/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0780 - accuracy: 0.9739 - val_loss: 0.1042 - val_accuracy: 0.9650 - lr: 0.0050\n",
            "Epoch 108/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0680 - accuracy: 0.9772 - val_loss: 0.0817 - val_accuracy: 0.9735 - lr: 0.0050\n",
            "Epoch 109/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0685 - accuracy: 0.9770 - val_loss: 0.0925 - val_accuracy: 0.9695 - lr: 0.0050\n",
            "Epoch 110/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0707 - accuracy: 0.9763 - val_loss: 0.0869 - val_accuracy: 0.9715 - lr: 0.0050\n",
            "Epoch 111/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0696 - accuracy: 0.9767 - val_loss: 0.0811 - val_accuracy: 0.9737 - lr: 0.0050\n",
            "Epoch 112/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0671 - accuracy: 0.9776 - val_loss: 0.0870 - val_accuracy: 0.9714 - lr: 0.0050\n",
            "Epoch 113/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0675 - accuracy: 0.9773 - val_loss: 0.0786 - val_accuracy: 0.9746 - lr: 0.0050\n",
            "Epoch 114/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0646 - accuracy: 0.9783 - val_loss: 0.0817 - val_accuracy: 0.9734 - lr: 0.0050\n",
            "Epoch 115/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0775 - accuracy: 0.9744 - val_loss: 0.0857 - val_accuracy: 0.9719 - lr: 0.0050\n",
            "Epoch 116/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0712 - accuracy: 0.9763 - val_loss: 0.1102 - val_accuracy: 0.9629 - lr: 0.0050\n",
            "Epoch 117/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0666 - accuracy: 0.9777 - val_loss: 0.0932 - val_accuracy: 0.9692 - lr: 0.0050\n",
            "Epoch 118/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0652 - accuracy: 0.9781 - val_loss: 0.0792 - val_accuracy: 0.9743 - lr: 0.0050\n",
            "Epoch 119/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0678 - accuracy: 0.9775 - val_loss: 0.1051 - val_accuracy: 0.9645 - lr: 0.0050\n",
            "Epoch 120/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0632 - accuracy: 0.9788 - val_loss: 0.0744 - val_accuracy: 0.9761 - lr: 0.0050\n",
            "Epoch 121/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0619 - accuracy: 0.9793 - val_loss: 0.0814 - val_accuracy: 0.9736 - lr: 0.0050\n",
            "Epoch 122/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0687 - accuracy: 0.9771 - val_loss: 0.0773 - val_accuracy: 0.9749 - lr: 0.0050\n",
            "Epoch 123/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0603 - accuracy: 0.9798 - val_loss: 0.0779 - val_accuracy: 0.9748 - lr: 0.0050\n",
            "Epoch 124/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0624 - accuracy: 0.9790 - val_loss: 0.0763 - val_accuracy: 0.9753 - lr: 0.0050\n",
            "Epoch 125/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0675 - accuracy: 0.9776 - val_loss: 0.0946 - val_accuracy: 0.9686 - lr: 0.0050\n",
            "Epoch 126/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0599 - accuracy: 0.9800 - val_loss: 0.0750 - val_accuracy: 0.9759 - lr: 0.0050\n",
            "Epoch 127/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0682 - accuracy: 0.9775 - val_loss: 0.1100 - val_accuracy: 0.9665 - lr: 0.0050\n",
            "Epoch 128/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0618 - accuracy: 0.9792 - val_loss: 0.0738 - val_accuracy: 0.9763 - lr: 0.0050\n",
            "Epoch 129/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0617 - accuracy: 0.9795 - val_loss: 0.0841 - val_accuracy: 0.9724 - lr: 0.0050\n",
            "Epoch 130/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0618 - accuracy: 0.9793 - val_loss: 0.0726 - val_accuracy: 0.9767 - lr: 0.0050\n",
            "Epoch 131/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0581 - accuracy: 0.9805 - val_loss: 0.0735 - val_accuracy: 0.9764 - lr: 0.0050\n",
            "Epoch 132/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0656 - accuracy: 0.9782 - val_loss: 0.0714 - val_accuracy: 0.9771 - lr: 0.0050\n",
            "Epoch 133/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0642 - accuracy: 0.9787 - val_loss: 0.0711 - val_accuracy: 0.9773 - lr: 0.0050\n",
            "Epoch 134/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0831 - accuracy: 0.9729 - val_loss: 0.0874 - val_accuracy: 0.9713 - lr: 0.0050\n",
            "Epoch 135/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0602 - accuracy: 0.9801 - val_loss: 0.1295 - val_accuracy: 0.9589 - lr: 0.0050\n",
            "Epoch 136/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0582 - accuracy: 0.9806 - val_loss: 0.0720 - val_accuracy: 0.9768 - lr: 0.0050\n",
            "Epoch 137/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0837 - accuracy: 0.9723 - val_loss: 0.0827 - val_accuracy: 0.9726 - lr: 0.0050\n",
            "Epoch 138/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0622 - accuracy: 0.9791 - val_loss: 0.0739 - val_accuracy: 0.9759 - lr: 0.0050\n",
            "Epoch 139/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0612 - accuracy: 0.9795 - val_loss: 0.0729 - val_accuracy: 0.9763 - lr: 0.0050\n",
            "Epoch 140/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0622 - accuracy: 0.9792 - val_loss: 0.0721 - val_accuracy: 0.9767 - lr: 0.0050\n",
            "Epoch 141/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0621 - accuracy: 0.9793 - val_loss: 0.0718 - val_accuracy: 0.9767 - lr: 0.0050\n",
            "Epoch 142/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0559 - accuracy: 0.9814 - val_loss: 0.0802 - val_accuracy: 0.9738 - lr: 0.0050\n",
            "Epoch 143/200\n",
            "187/187 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9819\n",
            "Epoch 143: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0542 - accuracy: 0.9819 - val_loss: 0.0831 - val_accuracy: 0.9730 - lr: 0.0050\n",
            "Epoch 144/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0455 - accuracy: 0.9853 - val_loss: 0.0633 - val_accuracy: 0.9800 - lr: 0.0025\n",
            "Epoch 145/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0442 - accuracy: 0.9857 - val_loss: 0.0648 - val_accuracy: 0.9796 - lr: 0.0025\n",
            "Epoch 146/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0442 - accuracy: 0.9856 - val_loss: 0.0646 - val_accuracy: 0.9796 - lr: 0.0025\n",
            "Epoch 147/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0442 - accuracy: 0.9856 - val_loss: 0.0645 - val_accuracy: 0.9797 - lr: 0.0025\n",
            "Epoch 148/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0444 - accuracy: 0.9855 - val_loss: 0.0645 - val_accuracy: 0.9797 - lr: 0.0025\n",
            "Epoch 149/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0438 - accuracy: 0.9857 - val_loss: 0.0641 - val_accuracy: 0.9799 - lr: 0.0025\n",
            "Epoch 150/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0436 - accuracy: 0.9857 - val_loss: 0.0663 - val_accuracy: 0.9791 - lr: 0.0025\n",
            "Epoch 151/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0441 - accuracy: 0.9855 - val_loss: 0.0645 - val_accuracy: 0.9798 - lr: 0.0025\n",
            "Epoch 152/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0434 - accuracy: 0.9857 - val_loss: 0.0647 - val_accuracy: 0.9798 - lr: 0.0025\n",
            "Epoch 153/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0445 - accuracy: 0.9854 - val_loss: 0.0637 - val_accuracy: 0.9801 - lr: 0.0025\n",
            "Epoch 154/200\n",
            "187/187 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9858\n",
            "Epoch 154: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0431 - accuracy: 0.9858 - val_loss: 0.0648 - val_accuracy: 0.9798 - lr: 0.0025\n",
            "Epoch 155/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0377 - accuracy: 0.9880 - val_loss: 0.0605 - val_accuracy: 0.9814 - lr: 0.0012\n",
            "Epoch 156/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0371 - accuracy: 0.9882 - val_loss: 0.0610 - val_accuracy: 0.9813 - lr: 0.0012\n",
            "Epoch 157/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0369 - accuracy: 0.9883 - val_loss: 0.0615 - val_accuracy: 0.9811 - lr: 0.0012\n",
            "Epoch 158/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0369 - accuracy: 0.9882 - val_loss: 0.0606 - val_accuracy: 0.9815 - lr: 0.0012\n",
            "Epoch 159/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0374 - accuracy: 0.9880 - val_loss: 0.0611 - val_accuracy: 0.9813 - lr: 0.0012\n",
            "Epoch 160/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0368 - accuracy: 0.9882 - val_loss: 0.0617 - val_accuracy: 0.9811 - lr: 0.0012\n",
            "Epoch 161/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0367 - accuracy: 0.9882 - val_loss: 0.0613 - val_accuracy: 0.9812 - lr: 0.0012\n",
            "Epoch 162/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0368 - accuracy: 0.9881 - val_loss: 0.0615 - val_accuracy: 0.9812 - lr: 0.0012\n",
            "Epoch 163/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0363 - accuracy: 0.9884 - val_loss: 0.0627 - val_accuracy: 0.9808 - lr: 0.0012\n",
            "Epoch 164/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0365 - accuracy: 0.9883 - val_loss: 0.0615 - val_accuracy: 0.9813 - lr: 0.0012\n",
            "Epoch 165/200\n",
            "187/187 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9885\n",
            "Epoch 165: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0359 - accuracy: 0.9885 - val_loss: 0.0612 - val_accuracy: 0.9814 - lr: 0.0012\n",
            "Epoch 166/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0333 - accuracy: 0.9896 - val_loss: 0.0599 - val_accuracy: 0.9819 - lr: 6.2500e-04\n",
            "Epoch 167/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0329 - accuracy: 0.9898 - val_loss: 0.0600 - val_accuracy: 0.9819 - lr: 6.2500e-04\n",
            "Epoch 168/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0328 - accuracy: 0.9897 - val_loss: 0.0604 - val_accuracy: 0.9818 - lr: 6.2500e-04\n",
            "Epoch 169/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0328 - accuracy: 0.9898 - val_loss: 0.0607 - val_accuracy: 0.9818 - lr: 6.2500e-04\n",
            "Epoch 170/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.0603 - val_accuracy: 0.9819 - lr: 6.2500e-04\n",
            "Epoch 171/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0326 - accuracy: 0.9898 - val_loss: 0.0604 - val_accuracy: 0.9819 - lr: 6.2500e-04\n",
            "Epoch 172/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0326 - accuracy: 0.9898 - val_loss: 0.0610 - val_accuracy: 0.9817 - lr: 6.2500e-04\n",
            "Epoch 173/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0324 - accuracy: 0.9898 - val_loss: 0.0606 - val_accuracy: 0.9819 - lr: 6.2500e-04\n",
            "Epoch 174/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0323 - accuracy: 0.9899 - val_loss: 0.0605 - val_accuracy: 0.9819 - lr: 6.2500e-04\n",
            "Epoch 175/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0322 - accuracy: 0.9899 - val_loss: 0.0612 - val_accuracy: 0.9817 - lr: 6.2500e-04\n",
            "Epoch 176/200\n",
            "187/187 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9899\n",
            "Epoch 176: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0321 - accuracy: 0.9899 - val_loss: 0.0608 - val_accuracy: 0.9819 - lr: 6.2500e-04\n",
            "Epoch 177/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0306 - accuracy: 0.9906 - val_loss: 0.0602 - val_accuracy: 0.9822 - lr: 3.1250e-04\n",
            "Epoch 178/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0305 - accuracy: 0.9906 - val_loss: 0.0603 - val_accuracy: 0.9822 - lr: 3.1250e-04\n",
            "Epoch 179/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.0607 - val_accuracy: 0.9820 - lr: 3.1250e-04\n",
            "Epoch 180/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.0606 - val_accuracy: 0.9821 - lr: 3.1250e-04\n",
            "Epoch 181/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0303 - accuracy: 0.9907 - val_loss: 0.0606 - val_accuracy: 0.9821 - lr: 3.1250e-04\n",
            "Epoch 182/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0302 - accuracy: 0.9907 - val_loss: 0.0606 - val_accuracy: 0.9821 - lr: 3.1250e-04\n",
            "Epoch 183/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0302 - accuracy: 0.9907 - val_loss: 0.0606 - val_accuracy: 0.9822 - lr: 3.1250e-04\n",
            "Epoch 184/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0301 - accuracy: 0.9907 - val_loss: 0.0607 - val_accuracy: 0.9821 - lr: 3.1250e-04\n",
            "Epoch 185/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0300 - accuracy: 0.9907 - val_loss: 0.0608 - val_accuracy: 0.9821 - lr: 3.1250e-04\n",
            "Epoch 186/200\n",
            "187/187 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9908\n",
            "Epoch 186: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0300 - accuracy: 0.9908 - val_loss: 0.0611 - val_accuracy: 0.9820 - lr: 3.1250e-04\n",
            "Epoch 187/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0292 - accuracy: 0.9911 - val_loss: 0.0609 - val_accuracy: 0.9821 - lr: 1.5625e-04\n",
            "Epoch 188/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0291 - accuracy: 0.9912 - val_loss: 0.0607 - val_accuracy: 0.9822 - lr: 1.5625e-04\n",
            "Epoch 189/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0291 - accuracy: 0.9912 - val_loss: 0.0608 - val_accuracy: 0.9822 - lr: 1.5625e-04\n",
            "Epoch 190/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0291 - accuracy: 0.9912 - val_loss: 0.0608 - val_accuracy: 0.9822 - lr: 1.5625e-04\n",
            "Epoch 191/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0290 - accuracy: 0.9912 - val_loss: 0.0611 - val_accuracy: 0.9821 - lr: 1.5625e-04\n",
            "Epoch 192/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0290 - accuracy: 0.9912 - val_loss: 0.0609 - val_accuracy: 0.9822 - lr: 1.5625e-04\n",
            "Epoch 193/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0290 - accuracy: 0.9912 - val_loss: 0.0611 - val_accuracy: 0.9821 - lr: 1.5625e-04\n",
            "Epoch 194/200\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0289 - accuracy: 0.9912 - val_loss: 0.0609 - val_accuracy: 0.9822 - lr: 1.5625e-04\n",
            "Epoch 195/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0289 - accuracy: 0.9912 - val_loss: 0.0611 - val_accuracy: 0.9821 - lr: 1.5625e-04\n",
            "Epoch 196/200\n",
            "187/187 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9912\n",
            "Epoch 196: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
            "187/187 [==============================] - 46s 246ms/step - loss: 0.0289 - accuracy: 0.9912 - val_loss: 0.0610 - val_accuracy: 0.9822 - lr: 1.5625e-04\n",
            "Epoch 197/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0284 - accuracy: 0.9914 - val_loss: 0.0611 - val_accuracy: 0.9822 - lr: 7.8125e-05\n",
            "Epoch 198/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0284 - accuracy: 0.9915 - val_loss: 0.0611 - val_accuracy: 0.9822 - lr: 7.8125e-05\n",
            "Epoch 199/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0284 - accuracy: 0.9914 - val_loss: 0.0611 - val_accuracy: 0.9822 - lr: 7.8125e-05\n",
            "Epoch 200/200\n",
            "187/187 [==============================] - 46s 247ms/step - loss: 0.0284 - accuracy: 0.9915 - val_loss: 0.0611 - val_accuracy: 0.9822 - lr: 7.8125e-05\n",
            "Model Saved\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7e1ae905-cfc9-49da-aeb9-4007268e2559\", \"LSTM_model.h5\", 8877464)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 130, 33)]    0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        296960      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 512)          0           ['lstm[0][1]',                   \n",
            "                                                                  'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          65664       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 362,624\n",
            "Trainable params: 362,624\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          33024       ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          33024       ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,048\n",
            "Trainable params: 66,048\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(1, 1, 33)]              0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (1, 1, 256)               296960    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (1, 1, 33)                8481      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 305,441\n",
            "Trainable params: 305,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "test smiles (63798, 128)\n",
            "CC(C)(C)C1CCC2(CC1)CN(c1ccc(Oc\n",
            "CC(C)(C)C1CCC2(CC1)CN(c1ccc(F)cc1)C(=O)N2Cc1ccc(C(=O)Nc2nn[nH]n2)cc1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:119: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CC(C)(C)C1CCC2(CC1)CN(c1ccc(F)\n",
            "CC(C)(C)C1CCC2(CC1)CN(c1ccc(F)cc1)C(=O)N2Cc1ccc(C(=O)Nc2nn[nH]n2)cc1\n",
            "96.0 percent wrongly formatted smiles\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit-pypi -qqq\n",
        "from rdkit import Chem\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import pandas as pd\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(uploaded['gandata.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "smiles_train, smiles_test = train_test_split(data[\"canonical_smiles\"], random_state=42)\n",
        "print (smiles_train.shape)\n",
        "print (smiles_test.shape)\n",
        "\n",
        "charset = set(\"\".join(list(data.canonical_smiles))+\"!E\")\n",
        "char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
        "int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
        "embed = max([len(smile) for smile in data.canonical_smiles]) + 5\n",
        "print (str(charset))\n",
        "n_vocab=len(charset)\n",
        "\n",
        "def vectorize(smiles, embed, n_vocab):\n",
        "    one_hot = np.zeros((smiles.shape[0],embed, n_vocab), dtype=np.int8)\n",
        "    for i, smile in enumerate(smiles):\n",
        "        # encode the startchar\n",
        "        one_hot[i, 0, char_to_int[\"!\"]] = 1\n",
        "        # encode the rest of the chars\n",
        "        for j, c in enumerate(smile):\n",
        "            one_hot[i, j + 1, char_to_int[c]] = 1\n",
        "        # Encode endchar\n",
        "        one_hot[i, len(smile) + 1:, char_to_int[\"E\"]] = 1\n",
        "    # Return two, one for input and the other for output\n",
        "    return one_hot[:, 0:-1, :], one_hot[:, 1:, :]\n",
        "\n",
        "X_train, Y_train = vectorize(smiles_train.values,embed,n_vocab)\n",
        "X_test, Y_test = vectorize(smiles_test.values,embed,n_vocab)\n",
        "print(smiles_train.iloc[0])\n",
        "\n",
        "print (X_train.shape)\n",
        "q=\"\".join([int_to_char[idx] for idx in np.argmax(X_train[0,:,:], axis=1)])\n",
        "print(q)\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Concatenate\n",
        "from keras import regularizers\n",
        "\n",
        "def lstm_model(X,y):\n",
        "    enc_input=Input(shape=(X.shape[1:]))\n",
        "    _,state_h,state_c=LSTM(256,return_state=True)(enc_input)\n",
        "    states= Concatenate(axis=-1)([state_h,state_c])\n",
        "    bottle_neck= Dense(128, activation='relu')(states)\n",
        "\n",
        "    unroll = False\n",
        "    state_h_decoded= Dense(256, activation='relu')(bottle_neck)\n",
        "    state_c_decoded= Dense(256, activation='relu')(bottle_neck)\n",
        "    encoder_states= [state_h_decoded, state_c_decoded]\n",
        "    dec_input= Input(shape=(X.shape[1:]))\n",
        "    dec1= LSTM(256, return_sequences=True,unroll=unroll)\n",
        "    decoder_outputs = dec1(dec_input, initial_state=encoder_states)\n",
        "    decoder_dense= Dense (y.shape[2], activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model= Model(inputs=[enc_input, dec_input], outputs=decoder_outputs)\n",
        "    return model\n",
        "\n",
        "model = lstm_model(X_train, Y_train)\n",
        "model.summary()\n",
        "\n",
        "from keras.callbacks import History, ReduceLROnPlateau\n",
        "h = History()\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=10, min_lr=0.000001, verbose=1, min_delta=1e-5)\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "opt=Adam(learning_rate=0.005) #Default 0.001\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit([X_train,X_train],Y_train,epochs=200,batch_size=1024,shuffle=True,callbacks=[h, rlr],validation_data=([X_test,X_test],Y_test ))\n",
        "mod_file = 'LSTM_model.h5'\n",
        "model.save(mod_file)\n",
        "print(\"Model Saved\")\n",
        "files.download(mod_file)\n",
        "\n",
        "encoder_model=Model(inputs=model.layers[0].input, outputs=model.layers[3].output)\n",
        "encoder_model.summary()\n",
        "\n",
        "latent_input = Input(shape=(128,))\n",
        "#reuse_layers\n",
        "state_h =  model.layers[5](latent_input)\n",
        "state_c =  model.layers[6](latent_input)\n",
        "latent_to_states_model = Model(latent_input, [state_h, state_c])\n",
        "latent_to_states_model.summary()\n",
        "\n",
        "\n",
        "#Last one is special, we need to change it to stateful, and change the input shape\n",
        "decoder_inputs = Input(batch_shape=(1, 1, n_vocab))\n",
        "decoder_lstm = LSTM(256,\n",
        "                    return_sequences=True,\n",
        "                    stateful=True\n",
        "                   )(decoder_inputs)\n",
        "decoder_outputs = Dense(n_vocab, activation='softmax')(decoder_lstm)\n",
        "gen_model = Model(decoder_inputs, decoder_outputs)\n",
        "\n",
        "#Transfer Weights\n",
        "for i in range(1,3):\n",
        "    gen_model.layers[i].set_weights(model.layers[i+6].get_weights())\n",
        "gen_model.save(\"Blog_simple_samplemodel.h5\")\n",
        "gen_model.summary()\n",
        "\n",
        "def sample_with_temp(preds, sampling_temp):\n",
        "    streched= np.log(preds)/ sampling_temp\n",
        "    streched_probs=np.exp(streched)/np.sum(np.exp(streched))\n",
        "    return np.random.choice(range(len(streched)),p=streched_probs )\n",
        "\n",
        "def sample_smiles(latent,sampling_temp):\n",
        "    #decode states and set Reset the LSTM cells with them\n",
        "    states = latent_to_states_model.predict(latent)\n",
        "    gen_model.layers[1].reset_states(states=[states[0],states[1]])\n",
        "    #Prepare the input char\n",
        "    startidx = char_to_int[\"!\"]\n",
        "    samplevec = np.zeros((1,1,n_vocab))\n",
        "    samplevec[0,0,startidx] = 1\n",
        "    sequence = \"\"\n",
        "    #Loop and predict next char\n",
        "    for i in range(30):\n",
        "        preds = gen_model.predict(samplevec)[0][-1]\n",
        "        if sampling_temp==1.0:\n",
        "            \n",
        "            sampleidx = np.argmax(preds)\n",
        "        else:\n",
        "            sampleidx=sample_with_temp(preds, sampling_temp)\n",
        "        samplechar = int_to_char[sampleidx]\n",
        "        if samplechar != \"E\":\n",
        "            sequence += samplechar\n",
        "            samplevec = np.zeros((1,1,n_vocab))\n",
        "            samplevec[0,0,sampleidx] = 1\n",
        "        else:\n",
        "            break\n",
        "    return sequence\n",
        "\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "import pandas as pd\n",
        "import io\n",
        "#data1 = pd.read_csv(io.BytesIO(uploaded['bioactivity_data.csv']))\n",
        "#smiles_t= data1[\"canonical_smiles\"]\n",
        "#charset = set(\"\".join(list(data1.canonical_smiles))+\"!E\")\n",
        "#char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
        "#int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
        "#embed = max([len(smile) for smile in data1.canonical_smiles]) + 5\n",
        "#print (str(charset))\n",
        "#n_vocab1=len(charset)\n",
        "#print(n_vocab1)\n",
        "#X_t, Y_t = vectorize(smiles_t.values,embed,n_vocab1)\n",
        "\n",
        "test_latent_space=encoder_model.predict(X_test)\n",
        "print(f'test smiles {test_latent_space.shape}')\n",
        "\n",
        "sampling_temp= 1.0\n",
        "test_smile= sample_smiles(test_latent_space[0:1], sampling_temp)\n",
        "print(test_smile)\n",
        "print(smiles_test.iloc[0])\n",
        "\n",
        "sampling_temp= 1.25\n",
        "test_smile= sample_smiles(test_latent_space[0:1], sampling_temp)\n",
        "print(test_smile)\n",
        "print(smiles_test.iloc[0])\n",
        "\n",
        "from rdkit import Chem\n",
        "#from rdkit.Chem import Draw, Descriptors\n",
        "sampling_temp=1.0\n",
        "wrong = 0\n",
        "for i in range(100):\n",
        "    smiles = sample_smiles(test_latent_space[i:i+1],sampling_temp)\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        pass\n",
        "    else:\n",
        "        \n",
        "        wrong = wrong + 1\n",
        "print (\"%0.1F percent wrongly formatted smiles\"%(wrong/float(100)*100))\n",
        "\n",
        "def generate(latent_seed,sampling_temp,scale,quant):\n",
        "  samples,mols=[],[]\n",
        "  for i in range(quant):\n",
        "    latent_vec=latent_seed+scale*(np.random.randn(latent_seed.shape[1]))\n",
        "    out= sample_smiles(latent_vec,sampling_temp)\n",
        "    mol= Chem.MolFromSmiles(out)\n",
        "    if mol:\n",
        "      mols.append(mol)\n",
        "      samples.append(out)\n",
        "  return mols, samples\n",
        "\n",
        "gen_mols, gen_smiles=[],[]\n",
        "for i in range(test_latent_space.shape[0]-1):\n",
        "  latent_seed=  test_latent_space[i:i+1]\n",
        "  sampling_temp=np.random.uniform(0.75,1.26)\n",
        "  scale=0.75\n",
        "  quantity=2\n",
        "  mols, smiles= generate(latent_seed, sampling_temp,scale,quantity)\n",
        "  gen_mols.extend(mols)\n",
        "  gen_smiles.extend(smiles)\n",
        "  mols,smiles=[],[]\n",
        "print(gen_smiles)\n",
        "\n",
        "from google.colab import files\n",
        "s = \"\\n\"\n",
        "\n",
        "str1 = s.join(gen_smiles)\n",
        "with open('example.txt', 'w') as f:\n",
        "  f.write(str1)\n",
        "\n",
        "files.download('example.txt')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVrrQAT9ntP5"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}